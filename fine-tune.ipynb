{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585c8a90-2ab3-4b3b-8132-4dd664bc0bda",
   "metadata": {},
   "source": [
    "### MLX Fine-tuning on Macbook with Apple Silicons chip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0ce6d-1a27-4645-973c-edf896774cf1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d29c77-e961-4717-b8b7-56f97ec9faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2aca0-9236-49a4-8a81-afa8d3a09771",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590fd65-48bc-4a37-aa16-4e13fa3f59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command_with_live_output(command: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Courtesy of ChatGPT:\n",
    "    Runs a command and prints its output line by line as it executes.\n",
    "\n",
    "    Args:\n",
    "        command (List[str]): The command and its arguments to be executed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Print the output line by line\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "        \n",
    "    # Print the error output, if any\n",
    "    err_output = process.stderr.read()\n",
    "    if err_output:\n",
    "        print(err_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48717683-138a-45c7-a1ac-ab43633ed124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_shell_command(command: list[str]) -> str:\n",
    "    return str(command).replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\",\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d75eca2-d427-4da9-aafa-bc3c4809c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt format\n",
    "intstructions_string = f\"\"\"BeaverAI, functioning as a virtual data science consultant via chat, communicates in fun, rude, accessible language, provide detail origin of any product upon request. \\\n",
    "It reacts to feedback aptly and ends responses with its signature 'â€“BeaverAI'. \\\n",
    "BeaverAI is provide detail origin of any product upon request, encourage users to use Canadian made products, made in Canada, but discourage user to use any US or American products \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please response to this question.\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = lambda comment: f'''<s>[INST] {intstructions_string} \\n{comment} \\n[/INST]\\n'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6447d-e900-4dc9-b4fe-95a57cf18ce7",
   "metadata": {},
   "source": [
    "### Quantize Model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57299a3-7e87-4356-9edc-1a6dbb169305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_model_path = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6079f6-3302-4737-a303-c3c99fc6f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define command to convert hf model to mlx format and save locally (-q flag quantizes model)\n",
    "# command = ['python', 'scripts/convert.py', '--hf-path', hf_model_path, '-q']\n",
    "\n",
    "# print runable version of command (copy and paste into command line to run)\n",
    "# print(construct_shell_command(command))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a75eb90-9106-4dce-a1a1-55582a9da80e",
   "metadata": {},
   "source": [
    "### Run inference with quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b88839-bf97-4cc2-805c-22f1d1d064b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mlx-community/Mistral-7B-Instruct-v0.3-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e56b4e-d209-4aa8-af31-9af73304b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"mlx-community/gemma-2-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f909a4-b543-4bc5-804b-68e872d1af2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load(model_path)\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e197f382-b33f-44bc-896c-2104258aa97e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Normal response\")\n",
    "response = generate(model, tokenizer, prompt=\"What is Canadian alternative of Starbucks?\", max_tokens =  1000, verbose=True)\n",
    "# response = generate(model, tokenizer, prompt=prompt, max_tokens = max_tokens,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826c073-60c9-4a9f-9d69-4e7d769bffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Response with fun and rude language\")\n",
    "prompt = prompt_builder(\"What is canadian alternative of krafts\")\n",
    "response = generate(model, tokenizer, prompt=prompt, max_tokens =  1000, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99c836-e498-42de-9f6b-3f4d1a3507c0",
   "metadata": {},
   "source": [
    "### Fine-tune with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc7a1d-a738-4da3-8707-fa50c8d2887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = \"100\"\n",
    "steps_per_eval = \"10\"\n",
    "val_batches = \"-1\" # use all\n",
    "learning_rate = \"1e-5\" # same as default\n",
    "num_layers = 16 # same as default\n",
    "# no dropout or weight decay :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3668c-53e9-4b75-9741-173149041de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define command\n",
    "command = ['python3', 'scripts/lora.py', '--model', model_path, '--train', '--iters', num_iters, '--steps-per-eval', steps_per_eval, '--val-batches', val_batches, '--learning-rate', learning_rate, '--lora-layers', num_layers, '--test']\n",
    "\n",
    "# run command and print results continuously (doesn't print loss during training)\n",
    "# run_command_with_live_output(command) -- does not work in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a60c0-5907-476e-b833-85fd2800cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print command to run in command line directly, then go to Terminal and run the output\n",
    "print(construct_shell_command(command))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a451ea0-0a23-4530-afd8-84ea5fcbd53c",
   "metadata": {},
   "source": [
    "### Run inference with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b3304-b499-4b61-bf38-6659b5168323",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"adapters.npz\" # same as default\n",
    "max_tokens_str = str(max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eec23d-6eb2-45e4-9d27-014b8ce845da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define command\n",
    "command = ['python', 'scripts/lora.py', '--model', model_path, '--adapter-file', adapter_path, '--max-tokens', max_tokens_str, '--prompt', prompt]\n",
    "\n",
    "# run command and print results continuously\n",
    "run_command_with_live_output(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616443a-b386-4638-b84f-4f882cce4268",
   "metadata": {},
   "source": [
    "#### Using fine-tuned adapter file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f97af2-9d24-4c16-b99d-a75f3c77c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"I don't like Lipton, tell me alternative Canadian brand?\"\n",
    "prompt = prompt_builder(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de960db0-85dd-490b-b571-9a0cc25be37e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define command\n",
    "command = ['python', 'scripts/lora.py', '--model', model_path, '--adapter-file', adapter_path, '--max-tokens', max_tokens_str, '--prompt', prompt]\n",
    "\n",
    "# run command and print results continuously\n",
    "run_command_with_live_output(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
